#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç ´å¤œç»˜æ˜æ¿€å…‰ç›‘æµ‹å¹³å° - æ¨¡å‹è®­ç»ƒä¸»å…¥å£
==============================================

ç›®å½•ç»“æ„:
    training/
    â”œâ”€â”€ train_main.py          # æœ¬æ–‡ä»¶
    â”œâ”€â”€ train_mac.sh           # Macè®­ç»ƒè„šæœ¬
    â”œâ”€â”€ configs/               # è®­ç»ƒé…ç½®
    â”œâ”€â”€ logs/                  # è®­ç»ƒæ—¥å¿—
    â”œâ”€â”€ checkpoints/           # æ£€æŸ¥ç‚¹æ–‡ä»¶
    â”‚   â”œâ”€â”€ transformer/
    â”‚   â”œâ”€â”€ switch/
    â”‚   â”œâ”€â”€ busbar/
    â”‚   â”œâ”€â”€ capacitor/
    â”‚   â””â”€â”€ meter/
    â”œâ”€â”€ exports/               # ONNXä¸´æ—¶å¯¼å‡º
    â”œâ”€â”€ data/                  # è®­ç»ƒæ•°æ®
    â””â”€â”€ results/               # è®­ç»ƒç»“æœ

ä½¿ç”¨æ–¹æ³•:
    # ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
    python training/train_main.py --mode demo

    # è®­ç»ƒå•ä¸ªæ’ä»¶
    python training/train_main.py --mode plugin --plugin transformer --epochs 30

    # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
    python training/train_main.py --mode all --epochs 50

ä½œè€…: ç ´å¤œç»˜æ˜å›¢é˜Ÿ
"""

import os
import sys
import json
import argparse
import logging
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import numpy as np

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•å’Œé¡¹ç›®æ ¹ç›®å½•
SCRIPT_DIR = Path(__file__).parent.resolve()
PROJECT_ROOT = SCRIPT_DIR.parent

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
LOG_DIR = SCRIPT_DIR / "logs"
LOG_DIR.mkdir(parents=True, exist_ok=True)

# é…ç½®æ—¥å¿— - ä¿å­˜åˆ° training/logs/
log_filename = LOG_DIR / f'training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(str(log_filename), encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# PyTorchå¯¼å…¥
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    TORCH_AVAILABLE = True
    logger.info(f"âœ… PyTorch {torch.__version__} å·²åŠ è½½")
except ImportError as e:
    TORCH_AVAILABLE = False
    logger.error(f"âŒ PyTorchæœªå®‰è£…: {e}")

# ONNXå¯¼å…¥
try:
    import onnx
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False

try:
    import onnxruntime as ort
    ORT_AVAILABLE = True
except ImportError:
    ORT_AVAILABLE = False


# =============================================================================
# å¹³å°æ£€æµ‹
# =============================================================================
def detect_platform():
    """æ£€æµ‹è¿è¡Œå¹³å°"""
    import platform
    
    system = platform.system()
    machine = platform.machine()
    
    info = {
        "system": system,
        "machine": machine,
        "device": "cpu",
        "device_name": "CPU"
    }
    
    if TORCH_AVAILABLE:
        # æ£€æµ‹MPS (Apple Silicon)
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            info["device"] = "mps"
            info["device_name"] = f"Apple Silicon MPS ({machine})"
        # æ£€æµ‹CUDA
        elif torch.cuda.is_available():
            info["device"] = "cuda"
            info["device_name"] = torch.cuda.get_device_name(0)
    
    return info


def get_device():
    """è·å–æœ€ä½³è®¾å¤‡"""
    if not TORCH_AVAILABLE:
        return None
    
    platform_info = detect_platform()
    device_type = platform_info["device"]
    
    if device_type == "mps":
        logger.info(f"âœ… ä½¿ç”¨ Apple Silicon MPS åŠ é€Ÿ")
        return torch.device("mps")
    elif device_type == "cuda":
        logger.info(f"âœ… ä½¿ç”¨ NVIDIA CUDA: {platform_info['device_name']}")
        return torch.device("cuda")
    else:
        logger.info("âš ï¸ ä½¿ç”¨ CPU è®­ç»ƒ")
        return torch.device("cpu")


# =============================================================================
# æ¨¡å‹å®šä¹‰ (ç®€åŒ–ç‰ˆï¼Œç¡®ä¿èƒ½è¿è¡Œ)
# =============================================================================
class SimpleDetectionModel(nn.Module):
    """ç®€åŒ–çš„æ£€æµ‹æ¨¡å‹"""
    def __init__(self, num_classes=10, input_size=(640, 640)):
        super().__init__()
        self.num_classes = num_classes
        
        self.backbone = nn.Sequential(
            nn.Conv2d(3, 32, 3, stride=2, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 64, 3, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, 3, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d(1)
        )
        
        self.head = nn.Linear(256, num_classes)
    
    def forward(self, x):
        x = self.backbone(x)
        x = x.view(x.size(0), -1)
        x = self.head(x)
        return x


class SimpleSegmentationModel(nn.Module):
    """ç®€åŒ–çš„åˆ†å‰²æ¨¡å‹"""
    def __init__(self, num_classes=2):
        super().__init__()
        
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
        )
        
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 2, stride=2),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 32, 2, stride=2),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, num_classes, 1)
        )
    
    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x


class SimpleClassificationModel(nn.Module):
    """ç®€åŒ–çš„åˆ†ç±»æ¨¡å‹"""
    def __init__(self, num_classes=4):
        super().__init__()
        
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 64, 3, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d(1)
        )
        
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128, 64),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(64, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x


# =============================================================================
# æ•°æ®é›† (ä¿®å¤ç‰ˆ)
# =============================================================================
class SimulatedDataset(Dataset):
    """æ¨¡æ‹Ÿæ•°æ®é›† - ç¡®ä¿ç”Ÿæˆæœ‰æ•ˆæ•°æ®"""
    
    def __init__(self, num_samples=500, input_size=(224, 224), 
                 num_classes=10, task="classification"):
        self.num_samples = num_samples
        self.input_size = input_size
        self.num_classes = num_classes
        self.task = task
        
        # é¢„ç”Ÿæˆæ‰€æœ‰æ•°æ®ä»¥ç¡®ä¿ä¸€è‡´æ€§
        logger.info(f"ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®é›†: {num_samples} æ ·æœ¬, {task}ä»»åŠ¡, {num_classes}ç±»")
        
        self.images = []
        self.labels = []
        
        for i in range(num_samples):
            # ç”Ÿæˆå¸¦æ¨¡å¼çš„å›¾åƒ (ä¸æ˜¯çº¯éšæœºå™ªå£°)
            img = self._generate_patterned_image(i)
            label = i % num_classes
            
            self.images.append(img)
            self.labels.append(label)
        
        logger.info(f"âœ… æ•°æ®é›†ç”Ÿæˆå®Œæˆ")
    
    def _generate_patterned_image(self, seed):
        """ç”Ÿæˆå¸¦æ¨¡å¼çš„å›¾åƒ"""
        np.random.seed(seed)
        
        # åŸºç¡€å›¾åƒ
        img = np.random.randint(50, 200, (*self.input_size, 3), dtype=np.uint8)
        
        # æ·»åŠ ä¸€äº›æ¨¡å¼ä½¿å…¶æœ‰åŒºåˆ†åº¦
        class_id = seed % self.num_classes
        
        # æ ¹æ®ç±»åˆ«æ·»åŠ ä¸åŒçš„å½¢çŠ¶
        h, w = self.input_size
        center_x, center_y = w // 2, h // 2
        radius = min(h, w) // 4
        
        # ç®€å•çš„åœ†å½¢/æ–¹å½¢æ¨¡å¼
        for y in range(h):
            for x in range(w):
                dist = np.sqrt((x - center_x)**2 + (y - center_y)**2)
                if dist < radius * (0.5 + 0.1 * class_id):
                    img[y, x] = [50 + class_id * 20, 100, 150]
        
        # å½’ä¸€åŒ–åˆ° [0, 1]
        img = img.astype(np.float32) / 255.0
        
        # æ ‡å‡†åŒ–
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        img = (img - mean) / std
        
        # è½¬æ¢ä¸º CHW æ ¼å¼
        img = img.transpose(2, 0, 1)
        
        return img.astype(np.float32)
    
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        image = torch.from_numpy(self.images[idx])
        label = torch.tensor(self.labels[idx], dtype=torch.long)
        return image, label


# =============================================================================
# è®­ç»ƒå™¨ (ä¿®å¤ç‰ˆ)
# =============================================================================
class Trainer:
    """ç®€åŒ–çš„è®­ç»ƒå™¨"""
    
    def __init__(self, model, device, save_dir):
        self.model = model.to(device)
        self.device = device
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        self.optimizer = optim.Adam(model.parameters(), lr=1e-3)
        self.criterion = nn.CrossEntropyLoss()
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.5)
        
        self.history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
    
    def train_epoch(self, dataloader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(dataloader):
            data, target = data.to(self.device), target.to(self.device)
            
            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output, target)
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()
        
        avg_loss = total_loss / len(dataloader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def validate(self, dataloader):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in dataloader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                loss = self.criterion(output, target)
                
                total_loss += loss.item()
                _, predicted = output.max(1)
                total += target.size(0)
                correct += predicted.eq(target).sum().item()
        
        avg_loss = total_loss / len(dataloader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def train(self, train_loader, val_loader, epochs, model_name):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        best_acc = 0
        
        logger.info(f"å¼€å§‹è®­ç»ƒ: {model_name}")
        logger.info(f"è®¾å¤‡: {self.device}")
        logger.info(f"Epochs: {epochs}")
        logger.info(f"ä¿å­˜ç›®å½•: {self.save_dir}")
        
        for epoch in range(epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_loss, val_acc = self.validate(val_loader)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            # è®°å½•å†å²
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_acc)
            
            logger.info(
                f"Epoch {epoch+1}/{epochs} | "
                f"Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | "
                f"Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%"
            )
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_acc:
                best_acc = val_acc
                self.save_checkpoint(model_name, epoch, val_acc, is_best=True)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_checkpoint(model_name, epochs, val_acc, is_best=False)
        
        logger.info(f"âœ… è®­ç»ƒå®Œæˆ! æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
        
        return self.history
    
    def save_checkpoint(self, model_name, epoch, accuracy, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'accuracy': accuracy,
            'history': self.history,
        }
        
        suffix = "best" if is_best else "final"
        save_path = self.save_dir / f"{model_name}_{suffix}.pth"
        
        torch.save(checkpoint, save_path)
        
        # éªŒè¯æ–‡ä»¶å·²ä¿å­˜
        if save_path.exists():
            size_kb = save_path.stat().st_size / 1024
            logger.info(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {save_path} ({size_kb:.1f} KB)")
        else:
            logger.error(f"âŒ ä¿å­˜å¤±è´¥: {save_path}")


# =============================================================================
# ONNXå¯¼å‡ºå™¨ (ä¿®å¤ç‰ˆ)
# =============================================================================
class ONNXExporter:
    """ONNXå¯¼å‡ºå™¨"""
    
    @staticmethod
    def export(model, input_size, save_path, model_name):
        """å¯¼å‡ºæ¨¡å‹ä¸ºONNX"""
        logger.info(f"å¯¼å‡ºONNX: {model_name}")
        
        model.eval()
        model = model.cpu()
        
        # åˆ›å»ºç›®å½•
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        
        # Dummyè¾“å…¥
        dummy_input = torch.randn(1, 3, *input_size)
        
        try:
            torch.onnx.export(
                model,
                dummy_input,
                save_path,
                opset_version=17,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # éªŒè¯æ–‡ä»¶
            if Path(save_path).exists():
                size_kb = Path(save_path).stat().st_size / 1024
                logger.info(f"âœ… ONNXå¯¼å‡ºæˆåŠŸ: {save_path} ({size_kb:.1f} KB)")
                
                # éªŒè¯ONNXæ¨¡å‹
                if ONNX_AVAILABLE:
                    onnx_model = onnx.load(save_path)
                    onnx.checker.check_model(onnx_model)
                    logger.info(f"âœ… ONNXæ¨¡å‹éªŒè¯é€šè¿‡")
                
                return True
            else:
                logger.error(f"âŒ ONNXæ–‡ä»¶æœªåˆ›å»º")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ONNXå¯¼å‡ºå¤±è´¥: {e}")
            return False


# =============================================================================
# æ¨¡å‹é…ç½®
# =============================================================================
PLUGIN_CONFIGS = {
    "transformer": {
        "description": "Aç»„ - ä¸»å˜å·¡è§†",
        "models": [
            {"name": "defect_yolov8n", "type": "detection", "input_size": (640, 640), "num_classes": 6},
            {"name": "oil_unet", "type": "segmentation", "input_size": (512, 512), "num_classes": 2},
            {"name": "silica_cnn", "type": "classification", "input_size": (224, 224), "num_classes": 4},
            {"name": "thermal_anomaly", "type": "classification", "input_size": (224, 224), "num_classes": 3},
        ]
    },
    "switch": {
        "description": "Bç»„ - å¼€å…³é—´éš”",
        "models": [
            {"name": "switch_yolov8s", "type": "detection", "input_size": (640, 640), "num_classes": 8},
            {"name": "indicator_ocr", "type": "classification", "input_size": (32, 128), "num_classes": 50},
        ]
    },
    "busbar": {
        "description": "Cç»„ - æ¯çº¿å·¡è§†",
        "models": [
            {"name": "busbar_yolov8m", "type": "detection", "input_size": (640, 640), "num_classes": 8},
            {"name": "noise_classifier", "type": "classification", "input_size": (128, 128), "num_classes": 5},
        ]
    },
    "capacitor": {
        "description": "Dç»„ - ç”µå®¹å™¨",
        "models": [
            {"name": "capacitor_yolov8", "type": "detection", "input_size": (640, 640), "num_classes": 6},
            {"name": "rtdetr_intrusion", "type": "detection", "input_size": (640, 640), "num_classes": 4},
        ]
    },
    "meter": {
        "description": "Eç»„ - è¡¨è®¡è¯»æ•°",
        "models": [
            {"name": "hrnet_keypoint", "type": "detection", "input_size": (256, 256), "num_classes": 8},
            {"name": "crnn_ocr", "type": "classification", "input_size": (32, 128), "num_classes": 37},
            {"name": "meter_classifier", "type": "classification", "input_size": (224, 224), "num_classes": 5},
        ]
    },
}


# =============================================================================
# è®­ç»ƒç®¡ç†å™¨
# =============================================================================
class TrainingManager:
    """è®­ç»ƒç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†è®­ç»ƒæ–‡ä»¶è¾“å‡º"""

    def __init__(self, base_dir=None):
        # ä½¿ç”¨ training/ ç›®å½•ä½œä¸ºåŸºç¡€ç›®å½•
        self.base_dir = Path(base_dir) if base_dir else SCRIPT_DIR
        self.project_root = PROJECT_ROOT
        self.device = get_device() if TORCH_AVAILABLE else None

        # åˆ›å»º training/ ä¸‹çš„ç›®å½•ç»“æ„
        self.dirs = {
            "checkpoints": self.base_dir / "checkpoints",      # æ£€æŸ¥ç‚¹
            "exports": self.base_dir / "exports",              # ONNXä¸´æ—¶å¯¼å‡º
            "logs": self.base_dir / "logs",                    # æ—¥å¿—
            "results": self.base_dir / "results",              # è®­ç»ƒç»“æœ
            "data": self.base_dir / "data",                    # è®­ç»ƒæ•°æ®
            "models_deploy": self.project_root / "models",     # éƒ¨ç½²æ¨¡å‹ç›®å½•
        }

        for name, d in self.dirs.items():
            d.mkdir(parents=True, exist_ok=True)
            if name != "models_deploy":
                logger.info(f"ğŸ“ è®­ç»ƒç›®å½•: {d}")

        self.results = {}
    
    def create_model(self, model_config):
        """åˆ›å»ºæ¨¡å‹"""
        model_type = model_config["type"]
        num_classes = model_config["num_classes"]
        input_size = model_config["input_size"]
        
        if model_type == "detection":
            return SimpleDetectionModel(num_classes, input_size)
        elif model_type == "segmentation":
            return SimpleSegmentationModel(num_classes)
        else:
            return SimpleClassificationModel(num_classes)
    
    def train_model(self, plugin_name, model_config, epochs=10, batch_size=16):
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        model_name = model_config["name"]
        input_size = model_config["input_size"]
        num_classes = model_config["num_classes"]
        
        logger.info(f"\n{'='*60}")
        logger.info(f"è®­ç»ƒ: {plugin_name}/{model_name}")
        logger.info(f"ç±»å‹: {model_config['type']}, è¾“å…¥: {input_size}, ç±»åˆ«: {num_classes}")
        logger.info(f"{'='*60}")
        
        # åˆ›å»ºæ¨¡å‹
        model = self.create_model(model_config)
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = SimulatedDataset(
            num_samples=500, 
            input_size=input_size, 
            num_classes=num_classes
        )
        val_dataset = SimulatedDataset(
            num_samples=100, 
            input_size=input_size, 
            num_classes=num_classes
        )
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        logger.info(f"è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        logger.info(f"éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
        
        # åˆ›å»ºè®­ç»ƒå™¨
        save_dir = self.dirs["checkpoints"] / plugin_name
        trainer = Trainer(model, self.device, save_dir)

        # è®­ç»ƒ
        history = trainer.train(train_loader, val_loader, epochs, model_name)

        # å¯¼å‡ºONNXåˆ°ä¸´æ—¶ç›®å½•
        export_dir = self.dirs["exports"] / plugin_name
        export_path = export_dir / f"{model_name}.onnx"
        ONNXExporter.export(model, input_size, str(export_path), model_name)

        # å¤åˆ¶åˆ°éƒ¨ç½²ç›®å½•
        deploy_dir = self.dirs["models_deploy"] / plugin_name
        deploy_dir.mkdir(parents=True, exist_ok=True)
        deploy_path = deploy_dir / f"{model_name}.onnx"

        if export_path.exists():
            import shutil
            shutil.copy2(str(export_path), str(deploy_path))
            logger.info(f"ğŸ“¦ å·²éƒ¨ç½²åˆ°: {deploy_path}")

        return {
            "status": "success",
            "history": history,
            "checkpoint": str(save_dir / f"{model_name}_best.pth"),
            "onnx_export": str(export_path),
            "onnx_deploy": str(deploy_path)
        }
    
    def train_plugin(self, plugin_name, epochs=10):
        """è®­ç»ƒæ’ä»¶æ‰€æœ‰æ¨¡å‹"""
        if plugin_name not in PLUGIN_CONFIGS:
            logger.error(f"æœªçŸ¥æ’ä»¶: {plugin_name}")
            return
        
        config = PLUGIN_CONFIGS[plugin_name]
        logger.info(f"\n{'#'*60}")
        logger.info(f"# è®­ç»ƒæ’ä»¶: {plugin_name} - {config['description']}")
        logger.info(f"{'#'*60}")
        
        results = {}
        for model_config in config["models"]:
            try:
                result = self.train_model(plugin_name, model_config, epochs)
                results[model_config["name"]] = result
            except Exception as e:
                logger.error(f"è®­ç»ƒå¤±è´¥ {model_config['name']}: {e}")
                import traceback
                traceback.print_exc()
                results[model_config["name"]] = {"status": "failed", "error": str(e)}
        
        self.results[plugin_name] = results
        return results
    
    def train_all(self, epochs=10):
        """è®­ç»ƒæ‰€æœ‰æ’ä»¶"""
        for plugin_name in PLUGIN_CONFIGS:
            self.train_plugin(plugin_name, epochs)

        # ä¿å­˜æ‘˜è¦åˆ° results/ ç›®å½•
        summary_path = self.dirs["results"] / "training_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)

        logger.info(f"\nâœ… è®­ç»ƒæ‘˜è¦å·²ä¿å­˜: {summary_path}")

        return self.results

    def run_demo(self, epochs=3):
        """æ¼”ç¤ºæ¨¡å¼"""
        logger.info("\n" + "="*60)
        logger.info("æ¼”ç¤ºæ¨¡å¼ - å¿«é€Ÿè®­ç»ƒæµ‹è¯•")
        logger.info("="*60)

        # åªè®­ç»ƒä¸€ä¸ªæ¨¡å‹ä½œä¸ºæ¼”ç¤º
        plugin_name = "transformer"
        model_config = PLUGIN_CONFIGS[plugin_name]["models"][2]  # silica_cnn

        result = self.train_model(plugin_name, model_config, epochs=epochs, batch_size=32)

        logger.info("\n" + "="*60)
        logger.info("æ¼”ç¤ºå®Œæˆ!")
        logger.info("="*60)
        logger.info(f"æ£€æŸ¥ç‚¹: {result['checkpoint']}")
        logger.info(f"ONNXå¯¼å‡º: {result['onnx_export']}")
        logger.info(f"ONNXéƒ¨ç½²: {result['onnx_deploy']}")

        # éªŒè¯æ–‡ä»¶
        if Path(result['checkpoint']).exists():
            logger.info(f"âœ… æ£€æŸ¥ç‚¹æ–‡ä»¶å­˜åœ¨: {Path(result['checkpoint']).stat().st_size / 1024:.1f} KB")
        if Path(result['onnx_deploy']).exists():
            logger.info(f"âœ… ONNXéƒ¨ç½²æ–‡ä»¶å­˜åœ¨: {Path(result['onnx_deploy']).stat().st_size / 1024:.1f} KB")

        return result


# =============================================================================
# å‘½ä»¤è¡Œæ¥å£
# =============================================================================
def main():
    parser = argparse.ArgumentParser(description="ç ´å¤œç»˜æ˜æ¿€å…‰ç›‘æµ‹å¹³å° - æ¨¡å‹è®­ç»ƒ")
    parser.add_argument("--mode", type=str, default="demo",
                       choices=["demo", "plugin", "all", "info"],
                       help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--plugin", type=str, default=None,
                       choices=list(PLUGIN_CONFIGS.keys()),
                       help="æŒ‡å®šæ’ä»¶")
    parser.add_argument("--epochs", type=int, default=10, help="è®­ç»ƒè½®æ•°")
    
    args = parser.parse_args()
    
    # æ‰“å°ç³»ç»Ÿä¿¡æ¯
    logger.info("="*60)
    logger.info("ç ´å¤œç»˜æ˜æ¿€å…‰ç›‘æµ‹å¹³å° - æ¨¡å‹è®­ç»ƒç³»ç»Ÿ")
    logger.info("="*60)
    
    platform_info = detect_platform()
    logger.info(f"ç³»ç»Ÿ: {platform_info['system']} ({platform_info['machine']})")
    logger.info(f"è®¾å¤‡: {platform_info['device']} - {platform_info['device_name']}")
    logger.info(f"PyTorch: {'å¯ç”¨' if TORCH_AVAILABLE else 'ä¸å¯ç”¨'}")
    logger.info(f"ONNX: {'å¯ç”¨' if ONNX_AVAILABLE else 'ä¸å¯ç”¨'}")
    logger.info(f"ONNX Runtime: {'å¯ç”¨' if ORT_AVAILABLE else 'ä¸å¯ç”¨'}")
    
    if not TORCH_AVAILABLE:
        logger.error("PyTorchæœªå®‰è£…ï¼Œæ— æ³•è®­ç»ƒ")
        return
    
    # åˆ›å»ºç®¡ç†å™¨
    manager = TrainingManager()
    
    if args.mode == "info":
        logger.info("\nå¯è®­ç»ƒçš„æ¨¡å‹:")
        for plugin, config in PLUGIN_CONFIGS.items():
            logger.info(f"\n{plugin} - {config['description']}:")
            for m in config["models"]:
                logger.info(f"  - {m['name']} ({m['type']}, {m['input_size']})")
    
    elif args.mode == "demo":
        manager.run_demo(epochs=args.epochs)
    
    elif args.mode == "plugin":
        if not args.plugin:
            logger.error("è¯·ä½¿ç”¨ --plugin æŒ‡å®šæ’ä»¶")
            return
        manager.train_plugin(args.plugin, epochs=args.epochs)
    
    elif args.mode == "all":
        manager.train_all(epochs=args.epochs)
    
    logger.info("\nâœ… å®Œæˆ!")


if __name__ == "__main__":
    main()
