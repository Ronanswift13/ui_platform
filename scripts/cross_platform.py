#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è·¨å¹³å°AIè®­ç»ƒä¸éƒ¨ç½²è„šæœ¬
ç ´å¤œç»˜æ˜æ¿€å…‰ç›‘æµ‹å¹³å°

åŠŸèƒ½:
- Mac Mç³»åˆ—èŠ¯ç‰‡ (MPSåŠ é€Ÿ) è®­ç»ƒ
- å¯¼å‡ºONNXæ¨¡å‹ (è·¨å¹³å°)
- Windows CUDA/TensorRT æ¨ç†

ä½¿ç”¨æ–¹æ³•:
    # Macä¸Šè®­ç»ƒ
    python cross_platform.py --mode train --device mps --epochs 50
    
    # å¯¼å‡ºONNX
    python cross_platform.py --mode export --model-path models/best.pth
    
    # Windowsæ¨ç†
    python cross_platform.py --mode inference --onnx-path models/model.onnx
"""

import os
import sys
import argparse
import platform
import logging
from pathlib import Path
from typing import Dict, Optional, Tuple
from dataclasses import dataclass

import numpy as np

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class PlatformConfig:
    """å¹³å°é…ç½®"""
    system: str           # Darwin, Windows, Linux
    device: str           # mps, cuda, cpu
    backend: str          # pytorch, onnx, tensorrt
    precision: str        # float32, float16
    batch_size: int
    is_training: bool


def detect_platform() -> PlatformConfig:
    """è‡ªåŠ¨æ£€æµ‹è¿è¡Œå¹³å°å¹¶è¿”å›æœ€ä¼˜é…ç½®"""
    system = platform.system()
    machine = platform.machine()
    
    logger.info(f"æ£€æµ‹åˆ°ç³»ç»Ÿ: {system} ({machine})")
    
    if system == "Darwin":
        # macOS
        try:
            import torch
            if torch.backends.mps.is_available():
                logger.info("âœ… Apple Silicon MPS å¯ç”¨")
                return PlatformConfig(
                    system="Darwin",
                    device="mps",
                    backend="pytorch",
                    precision="float32",  # MPSå½“å‰å¯¹FP16æ”¯æŒæœ‰é™
                    batch_size=16,
                    is_training=True
                )
        except ImportError:
            pass
        
        return PlatformConfig(
            system="Darwin",
            device="cpu",
            backend="pytorch",
            precision="float32",
            batch_size=8,
            is_training=True
        )
    
    elif system == "Windows":
        # Windows - ä¼˜å…ˆä½¿ç”¨CUDA
        try:
            import torch
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                logger.info(f"âœ… NVIDIA GPU å¯ç”¨: {gpu_name}")
                return PlatformConfig(
                    system="Windows",
                    device="cuda",
                    backend="onnx",  # Windowsæ¨èONNXæ¨ç†
                    precision="float16",
                    batch_size=32,
                    is_training=False
                )
        except ImportError:
            pass
        
        return PlatformConfig(
            system="Windows",
            device="cpu",
            backend="onnx",
            precision="float32",
            batch_size=4,
            is_training=False
        )
    
    else:
        # Linux
        return PlatformConfig(
            system="Linux",
            device="cpu",
            backend="pytorch",
            precision="float32",
            batch_size=8,
            is_training=True
        )


class CrossPlatformTrainer:
    """è·¨å¹³å°è®­ç»ƒå™¨"""
    
    def __init__(self, config: PlatformConfig):
        self.config = config
        self.device = self._get_torch_device()
        
    def _get_torch_device(self):
        """è·å–PyTorchè®¾å¤‡"""
        import torch
        
        if self.config.device == "mps":
            return torch.device("mps")
        elif self.config.device == "cuda":
            return torch.device("cuda")
        else:
            return torch.device("cpu")
    
    def train(self, model, train_loader, val_loader=None, 
              epochs: int = 50, save_path: str = "models/best.pth"):
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            model: PyTorchæ¨¡å‹
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            epochs: è®­ç»ƒè½®æ•°
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        """
        import torch
        import torch.nn as nn
        
        logger.info(f"å¼€å§‹è®­ç»ƒ: device={self.config.device}, epochs={epochs}")
        
        model = model.to(self.device)
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        criterion = nn.CrossEntropyLoss()
        
        best_val_acc = 0.0
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            for batch_idx, (data, target) in enumerate(train_loader):
                data, target = data.to(self.device), target.to(self.device)
                
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = output.max(1)
                train_total += target.size(0)
                train_correct += predicted.eq(target).sum().item()
            
            scheduler.step()
            
            train_acc = 100. * train_correct / train_total
            avg_loss = train_loss / len(train_loader)
            
            # éªŒè¯é˜¶æ®µ
            if val_loader is not None:
                val_acc = self._validate(model, val_loader)
                
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    self._save_checkpoint(model, save_path, epoch, val_acc)
                    logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: val_acc={val_acc:.2f}%")
                
                logger.info(f"Epoch {epoch+1}/{epochs}: "
                           f"loss={avg_loss:.4f}, train_acc={train_acc:.2f}%, "
                           f"val_acc={val_acc:.2f}%")
            else:
                logger.info(f"Epoch {epoch+1}/{epochs}: "
                           f"loss={avg_loss:.4f}, train_acc={train_acc:.2f}%")
        
        logger.info(f"âœ… è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
        return model
    
    def _validate(self, model, val_loader) -> float:
        """éªŒè¯æ¨¡å‹"""
        import torch
        
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = model(data)
                _, predicted = output.max(1)
                total += target.size(0)
                correct += predicted.eq(target).sum().item()
        
        return 100. * correct / total
    
    def _save_checkpoint(self, model, path: str, epoch: int, val_acc: float):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        import torch
        
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'val_acc': val_acc,
            'platform': self.config.system,
            'device': self.config.device
        }, path)


class ONNXExporter:
    """ONNXå¯¼å‡ºå™¨"""
    
    def __init__(self, opset_version: int = 17):
        self.opset_version = opset_version
    
    def export(self, model, input_shape: Tuple, 
               save_path: str, dynamic_batch: bool = True):
        """
        å¯¼å‡ºæ¨¡å‹ä¸ºONNXæ ¼å¼
        
        Args:
            model: PyTorchæ¨¡å‹
            input_shape: è¾“å…¥å½¢çŠ¶ (ä¸å«batchç»´åº¦)
            save_path: ONNXä¿å­˜è·¯å¾„
            dynamic_batch: æ˜¯å¦æ”¯æŒåŠ¨æ€batch size
        """
        import torch
        
        logger.info(f"å¯¼å‡ºONNXæ¨¡å‹: {save_path}")
        
        model.eval()
        model = model.to("cpu")  # ONNXå¯¼å‡ºéœ€è¦CPU
        
        # åˆ›å»ºdummyè¾“å…¥
        dummy_input = torch.randn(1, *input_shape)
        
        # åŠ¨æ€è½´é…ç½®
        dynamic_axes = None
        if dynamic_batch:
            dynamic_axes = {
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            }
        
        # å¯¼å‡º
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        torch.onnx.export(
            model,
            dummy_input,
            save_path,
            opset_version=self.opset_version,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes=dynamic_axes,
            do_constant_folding=True
        )
        
        logger.info(f"âœ… ONNXå¯¼å‡ºæˆåŠŸ: {save_path}")
        
        # éªŒè¯ONNX
        self._verify_onnx(save_path, dummy_input)
        
        # å¯é€‰: ç®€åŒ–ONNX
        self._simplify_onnx(save_path)
        
        return save_path
    
    def _verify_onnx(self, path: str, dummy_input):
        """éªŒè¯ONNXæ¨¡å‹"""
        import onnx
        import onnxruntime as ort
        
        # æ£€æŸ¥æ¨¡å‹ç»“æ„
        model = onnx.load(path)
        onnx.checker.check_model(model)
        logger.info("âœ… ONNXæ¨¡å‹éªŒè¯é€šè¿‡")
        
        # æµ‹è¯•æ¨ç†
        session = ort.InferenceSession(path)
        input_name = session.get_inputs()[0].name
        output = session.run(None, {input_name: dummy_input.numpy()})
        logger.info(f"âœ… ONNXæ¨ç†æµ‹è¯•é€šè¿‡, è¾“å‡ºå½¢çŠ¶: {output[0].shape}")
    
    def _simplify_onnx(self, path: str):
        """ç®€åŒ–ONNXæ¨¡å‹"""
        try:
            import onnxsim
            import onnx
            
            model = onnx.load(path)
            simplified, ok = onnxsim.simplify(model)
            
            if ok:
                onnx.save(simplified, path)
                logger.info("âœ… ONNXæ¨¡å‹ç®€åŒ–å®Œæˆ")
            else:
                logger.warning("âš ï¸ ONNXç®€åŒ–å¤±è´¥,ä¿ç•™åŸå§‹æ¨¡å‹")
        except ImportError:
            logger.info("â„¹ï¸ onnxsimæœªå®‰è£…,è·³è¿‡ç®€åŒ–æ­¥éª¤")


class CrossPlatformInference:
    """è·¨å¹³å°æ¨ç†å¼•æ“"""
    
    def __init__(self, model_path: str, use_gpu: bool = True):
        self.model_path = model_path
        self.session = None
        self._load_model(use_gpu)
    
    def _load_model(self, use_gpu: bool):
        """åŠ è½½ONNXæ¨¡å‹"""
        import onnxruntime as ort
        
        # é€‰æ‹©æ‰§è¡Œæä¾›è€…
        if use_gpu:
            providers = [
                ('TensorrtExecutionProvider', {
                    'device_id': 0,
                    'trt_fp16_enable': True
                }),
                ('CUDAExecutionProvider', {
                    'device_id': 0,
                    'cudnn_conv_algo_search': 'EXHAUSTIVE'
                }),
                'CPUExecutionProvider'
            ]
        else:
            providers = ['CPUExecutionProvider']
        
        sess_options = ort.SessionOptions()
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        
        self.session = ort.InferenceSession(
            self.model_path,
            sess_options=sess_options,
            providers=providers
        )
        
        actual_providers = self.session.get_providers()
        logger.info(f"âœ… ONNXæ¨¡å‹åŠ è½½å®Œæˆ, ä½¿ç”¨: {actual_providers}")
        
        self.input_name = self.session.get_inputs()[0].name
        self.output_names = [o.name for o in self.session.get_outputs()]
    
    def predict(self, input_data: np.ndarray) -> np.ndarray:
        """æ‰§è¡Œæ¨ç†"""
        if input_data.dtype != np.float32:
            input_data = input_data.astype(np.float32)
        
        outputs = self.session.run(self.output_names, {self.input_name: input_data})
        return outputs[0]
    
    def benchmark(self, input_shape: Tuple, n_iterations: int = 100) -> Dict:
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        import time
        
        dummy_input = np.random.randn(1, *input_shape).astype(np.float32)
        
        # é¢„çƒ­
        for _ in range(10):
            self.predict(dummy_input)
        
        # æµ‹è¯•
        times = []
        for _ in range(n_iterations):
            start = time.perf_counter()
            self.predict(dummy_input)
            times.append(time.perf_counter() - start)
        
        times = np.array(times) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        return {
            "mean_ms": float(np.mean(times)),
            "std_ms": float(np.std(times)),
            "min_ms": float(np.min(times)),
            "max_ms": float(np.max(times)),
            "fps": float(1000 / np.mean(times))
        }


def main():
    parser = argparse.ArgumentParser(description="è·¨å¹³å°AIè®­ç»ƒä¸éƒ¨ç½²")
    parser.add_argument("--mode", choices=["train", "export", "inference", "benchmark"],
                        default="train", help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--device", choices=["auto", "mps", "cuda", "cpu"],
                        default="auto", help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--epochs", type=int, default=50, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--model-path", type=str, default="models/best.pth",
                        help="PyTorchæ¨¡å‹è·¯å¾„")
    parser.add_argument("--onnx-path", type=str, default="models/model.onnx",
                        help="ONNXæ¨¡å‹è·¯å¾„")
    parser.add_argument("--input-shape", type=str, default="3,640,640",
                        help="è¾“å…¥å½¢çŠ¶ (C,H,W)")
    
    args = parser.parse_args()
    
    # è§£æè¾“å…¥å½¢çŠ¶
    input_shape = tuple(map(int, args.input_shape.split(",")))
    
    # æ£€æµ‹å¹³å°
    config = detect_platform()
    
    # è¦†ç›–è®¾å¤‡è®¾ç½®
    if args.device != "auto":
        config.device = args.device
    
    logger.info(f"è¿è¡Œé…ç½®: {config}")
    
    if args.mode == "train":
        # è®­ç»ƒæ¨¡å¼ (Mac)
        import torch
        import torch.nn as nn
        
        # ç¤ºä¾‹: åˆ›å»ºç®€å•æ¨¡å‹
        class SimpleModel(nn.Module):
            def __init__(self, in_channels=3, num_classes=10):
                super().__init__()
                self.features = nn.Sequential(
                    nn.Conv2d(in_channels, 32, 3, padding=1),
                    nn.ReLU(),
                    nn.MaxPool2d(2),
                    nn.Conv2d(32, 64, 3, padding=1),
                    nn.ReLU(),
                    nn.MaxPool2d(2),
                    nn.AdaptiveAvgPool2d(1)
                )
                self.classifier = nn.Linear(64, num_classes)
            
            def forward(self, x):
                x = self.features(x)
                x = x.view(x.size(0), -1)
                return self.classifier(x)
        
        model = SimpleModel()
        trainer = CrossPlatformTrainer(config)
        
        # åˆ›å»ºæ¼”ç¤ºæ•°æ®
        from torch.utils.data import DataLoader, TensorDataset
        train_data = torch.randn(100, *input_shape)
        train_labels = torch.randint(0, 10, (100,))
        train_loader = DataLoader(
            TensorDataset(train_data, train_labels),
            batch_size=config.batch_size,
            shuffle=True
        )
        
        trainer.train(model, train_loader, epochs=args.epochs, save_path=args.model_path)
        
    elif args.mode == "export":
        # å¯¼å‡ºONNX
        import torch
        
        checkpoint = torch.load(args.model_path, map_location="cpu")
        
        # é‡å»ºæ¨¡å‹ (éœ€è¦ä¸è®­ç»ƒæ—¶ä¸€è‡´)
        class SimpleModel(torch.nn.Module):
            def __init__(self, in_channels=3, num_classes=10):
                super().__init__()
                self.features = torch.nn.Sequential(
                    torch.nn.Conv2d(in_channels, 32, 3, padding=1),
                    torch.nn.ReLU(),
                    torch.nn.MaxPool2d(2),
                    torch.nn.Conv2d(32, 64, 3, padding=1),
                    torch.nn.ReLU(),
                    torch.nn.MaxPool2d(2),
                    torch.nn.AdaptiveAvgPool2d(1)
                )
                self.classifier = torch.nn.Linear(64, num_classes)
            
            def forward(self, x):
                x = self.features(x)
                x = x.view(x.size(0), -1)
                return self.classifier(x)
        
        model = SimpleModel()
        model.load_state_dict(checkpoint['model_state_dict'])
        
        exporter = ONNXExporter()
        exporter.export(model, input_shape, args.onnx_path)
        
    elif args.mode == "inference":
        # æ¨ç†æ¨¡å¼ (Windows)
        engine = CrossPlatformInference(args.onnx_path, use_gpu=config.device != "cpu")
        
        # ç¤ºä¾‹æ¨ç†
        dummy_input = np.random.randn(1, *input_shape).astype(np.float32)
        result = engine.predict(dummy_input)
        logger.info(f"æ¨ç†ç»“æœå½¢çŠ¶: {result.shape}")
        
    elif args.mode == "benchmark":
        # æ€§èƒ½æµ‹è¯•
        engine = CrossPlatformInference(args.onnx_path, use_gpu=config.device != "cpu")
        stats = engine.benchmark(input_shape)
        
        logger.info("æ€§èƒ½æµ‹è¯•ç»“æœ:")
        logger.info(f"  å¹³å‡å»¶è¿Ÿ: {stats['mean_ms']:.2f} ms")
        logger.info(f"  æ ‡å‡†å·®: {stats['std_ms']:.2f} ms")
        logger.info(f"  æœ€å°å»¶è¿Ÿ: {stats['min_ms']:.2f} ms")
        logger.info(f"  æœ€å¤§å»¶è¿Ÿ: {stats['max_ms']:.2f} ms")
        logger.info(f"  FPS: {stats['fps']:.1f}")


if __name__ == "__main__":
    main()
